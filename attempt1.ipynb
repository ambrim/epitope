{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "af5d6f2e",
      "metadata": {
        "id": "af5d6f2e"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install ðŸ¤— Transformers as well as some other libraries. Uncomment the following cell and run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "4c5bf8d4",
      "metadata": {
        "id": "4c5bf8d4"
      },
      "outputs": [],
      "source": [
        "#pip install transformers evaluate datasets requests pandas sklearn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76e71a3f",
      "metadata": {
        "id": "76e71a3f"
      },
      "source": [
        "If you're opening this notebook locally, make sure your environment has an install from the last version of those libraries.\n",
        "\n",
        "To be able to share your model with the community and generate results like the one shown in the picture below via the inference API, there are a few more steps to follow.\n",
        "\n",
        "First you have to store your authentication token from the Hugging Face website (sign up here if you haven't already!) then execute the following cell and input your username and password:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "25b8526a",
      "metadata": {
        "id": "25b8526a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ab972723ff834619b1f5a4c254e1b450",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab8b2712",
      "metadata": {
        "id": "ab8b2712"
      },
      "source": [
        "Then you need to install Git-LFS. Uncomment the following instructions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "19e8f77c",
      "metadata": {
        "id": "19e8f77c"
      },
      "outputs": [],
      "source": [
        "# !apt install git-lfs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80dbad4e",
      "metadata": {
        "id": "80dbad4e"
      },
      "source": [
        "We also quickly upload some telemetry - this tells us which examples and software versions are getting used so we know where to prioritize our maintenance efforts. We don't collect (or care about) any personally identifiable information, but if you'd prefer not to be counted, feel free to skip this step or delete this cell entirely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d107b8d9",
      "metadata": {
        "id": "d107b8d9"
      },
      "outputs": [],
      "source": [
        "# from transformers.utils import send_example_telemetry\n",
        "\n",
        "# send_example_telemetry(\"protein_language_modeling_notebook\", framework=\"pytorch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0749e1",
      "metadata": {
        "id": "5c0749e1"
      },
      "source": [
        "# Fine-Tuning Protein Language Models"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d81db83",
      "metadata": {
        "id": "1d81db83"
      },
      "source": [
        "In this notebook, we're going to do some transfer learning to fine-tune some large, pre-trained protein language models on tasks of interest. If that sentence feels a bit intimidating to you, don't panic - there's [a blog post](https://huggingface.co/blog/deep-learning-with-proteins) that explains the concepts here in much more detail.\n",
        "\n",
        "The specific model we're going to use is ESM-2, which is the state-of-the-art protein language model at the time of writing (November 2022). The citation for this model is [Lin et al, 2022](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v1).\n",
        "\n",
        "There are several ESM-2 checkpoints with differing model sizes. Larger models will generally have better accuracy, but they require more GPU memory and will take much longer to train. The available ESM-2 checkpoints (at time of writing) are:\n",
        "\n",
        "| Checkpoint name | Num layers | Num parameters |\n",
        "|------------------------------|----|----------|\n",
        "| `esm2_t48_15B_UR50D`         | 48 | 15B     |\n",
        "| `esm2_t36_3B_UR50D`          | 36 | 3B      |\n",
        "| `esm2_t33_650M_UR50D`        | 33 | 650M    |\n",
        "| `esm2_t30_150M_UR50D`        | 30 | 150M    |\n",
        "| `esm2_t12_35M_UR50D`         | 12 | 35M     |\n",
        "| `esm2_t6_8M_UR50D`           | 6  | 8M      |\n",
        "\n",
        "Note that the larger checkpoints may be very difficult to train without a large cloud GPU like an A100 or H100, and the largest 15B parameter checkpoint will probably be impossible to train on **any** single GPU! Also, note that memory usage for attention during training will scale as `O(batch_size * num_layers * seq_len^2)`, so larger models on long sequences will use quite a lot of memory! We will use the `esm2_t12_35M_UR50D` checkpoint for this notebook, which should train on any Colab instance or modern GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "32e605a2",
      "metadata": {
        "id": "32e605a2"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"facebook/esm2_t12_35M_UR50D\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3eb400c",
      "metadata": {
        "id": "c3eb400c"
      },
      "source": [
        "One of the most common tasks you can perform with a language model is **sequence classification**. In sequence classification, we classify an entire protein into a category, from a list of two or more possibilities. There's no limit on the number of categories you can use, or the specific problem you choose, as long as it's something the model could in theory infer from the raw protein sequence. To keep things simple for this example, though, let's try classifying proteins by their cellular localization - given their sequence, can we predict if they're going to be found in the cytosol (the fluid inside the cell) or embedded in the cell membrane?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c91d394",
      "metadata": {
        "id": "4c91d394"
      },
      "source": [
        "In this section, we're going to gather some training data from UniProt. Our goal is to create a pair of lists: `sequences` and `labels`. `sequences` will be a list of protein sequences, which will just be strings like \"MNKL...\", where each letter represents a single amino acid in the complete protein. `labels` will be a list of the category for each sequence. The categories will just be integers, with 0 representing the first category, 1 representing the second and so on. In other words, if `sequences[i]` is a protein sequence then `labels[i]` should be its corresponding category. These will form the **training data** we're going to use to teach the model the task we want it to do.\n",
        "\n",
        "If you're adapting this notebook for your own use, this will probably be the main section you want to change! You can do whatever you want here, as long as you create those two lists by the end of it. If you want to follow along with this example, though, first we'll need to `import requests` and set up our query to UniProt."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc2ef458",
      "metadata": {
        "id": "bc2ef458"
      },
      "source": [
        "***\n",
        "# Token classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "78d701ed",
      "metadata": {
        "id": "78d701ed"
      },
      "source": [
        "Another common language model task is **token classification**. In this task, instead of classifying the whole sequence into a single category, we categorize each token (amino acid, in this case!) into one or more categories. This kind of model could be useful for:\n",
        "\n",
        "- Predicting secondary structure\n",
        "- Predicting buried vs. exposed residues\n",
        "- Predicting residues that will receive post-translational modifications\n",
        "- Predicting residues involved in binding pockets or active sites\n",
        "- Probably several other things, it's been a while since I was a postdoc"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20e00afe",
      "metadata": {
        "id": "20e00afe"
      },
      "source": [
        "## Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f1b9e75c",
      "metadata": {
        "id": "f1b9e75c"
      },
      "source": [
        "In this section, we're going to gather some training data from UniProt. As in the sequence classification example, we aim to create two lists: `sequences` and `labels`. Unlike in that example, however, the `labels` are more than just single integers. Instead, the label for each sample will be **one integer per token in the input**. This should make sense - when we do token classification, different tokens in the input may have different categories!\n",
        "\n",
        "To demonstrate token classification, we're going to go back to UniProt and get some data on protein secondary structures. As above, this will probably the main section you want to change when adapting this code to your own problems."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4457b1a0",
      "metadata": {
        "id": "4457b1a0"
      },
      "source": [
        "Looks good! We can use this to build our training data. Recall that the **labels** need to be a list or array of integers that's the same length as the input sequence. We're going to use 0 to indicate residues without any annotated structure, 1 for residues in an alpha helix, and 2 for residues in a beta strand. To build that, we'll start with an array of all 0s, and then fill in values based on the positions that our regex pulls out of the UniProt results.\n",
        "\n",
        "We'll use NumPy arrays rather than lists here, since these allow [slice assignment](https://numpy.org/doc/stable/user/basics.indexing.html#assigning-values-to-indexed-arrays), which will be a lot simpler than editing a list of integers. Note also that UniProt annotates residues starting from 1 (unlike Python, which starts from 0), and region annotations are inclusive (so 1..3 means residues 1, 2 and 3). To turn these into Python slices, we subtract 1 from the start of each annotation, but not the end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Epitope ID - IEDB IRI</th>\n",
              "      <th>Epitope - Name</th>\n",
              "      <th>Epitope - Starting Position</th>\n",
              "      <th>Epitope - Ending Position</th>\n",
              "      <th>Epitope - Source Molecule</th>\n",
              "      <th>Epitope - Source Molecule IRI</th>\n",
              "      <th>Epitope ID</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>https://www.iedb.org/epitope/34</td>\n",
              "      <td>AAAGDK</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>B13 antigen</td>\n",
              "      <td>PPPFGQAAAGDKPSPFGQAAAGDKPPPFGQAAAGDKPSPFGQAAAG...</td>\n",
              "      <td>AAP88022.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>https://www.iedb.org/epitope/56</td>\n",
              "      <td>AAARTTS</td>\n",
              "      <td>7.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>envelope glycoprotein 2</td>\n",
              "      <td>TYATGGAAARTTSGFASLFSPGSAQ</td>\n",
              "      <td>ABP93610.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>https://www.iedb.org/epitope/60</td>\n",
              "      <td>AAASAIQGNVTSIHSL</td>\n",
              "      <td>13.0</td>\n",
              "      <td>28.0</td>\n",
              "      <td>6 kDa early secretory antigenic target</td>\n",
              "      <td>MTEQQWNFAGIEAAASAIQGNVTSIHSLLDEGKQSLTKLAAAWGGS...</td>\n",
              "      <td>P0A564.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>https://www.iedb.org/epitope/101</td>\n",
              "      <td>AAECPFLPKPKVA</td>\n",
              "      <td>241.0</td>\n",
              "      <td>253.0</td>\n",
              "      <td>nucleoprotein</td>\n",
              "      <td>MSTLQELQENITAHEQQLVTARQKLKDAEKAVEVDPDDVNKSTLQN...</td>\n",
              "      <td>AAO86636.1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8</td>\n",
              "      <td>https://www.iedb.org/epitope/143</td>\n",
              "      <td>AAGAGPRVRQ + NAc(A1)</td>\n",
              "      <td>69.0</td>\n",
              "      <td>78.0</td>\n",
              "      <td>ORF 2</td>\n",
              "      <td>MRPRPILLLLLMFLPMLPAPPPGQPSGRRRGRRSGGSGGGFWGDRA...</td>\n",
              "      <td>AAA03191.1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0             Epitope ID - IEDB IRI        Epitope - Name  \\\n",
              "0           0   https://www.iedb.org/epitope/34                AAAGDK   \n",
              "1           1   https://www.iedb.org/epitope/56               AAARTTS   \n",
              "2           2   https://www.iedb.org/epitope/60      AAASAIQGNVTSIHSL   \n",
              "3           6  https://www.iedb.org/epitope/101         AAECPFLPKPKVA   \n",
              "4           8  https://www.iedb.org/epitope/143  AAGAGPRVRQ + NAc(A1)   \n",
              "\n",
              "   Epitope - Starting Position  Epitope - Ending Position  \\\n",
              "0                          7.0                       12.0   \n",
              "1                          7.0                       13.0   \n",
              "2                         13.0                       28.0   \n",
              "3                        241.0                      253.0   \n",
              "4                         69.0                       78.0   \n",
              "\n",
              "                Epitope - Source Molecule  \\\n",
              "0                             B13 antigen   \n",
              "1                 envelope glycoprotein 2   \n",
              "2  6 kDa early secretory antigenic target   \n",
              "3                           nucleoprotein   \n",
              "4                                   ORF 2   \n",
              "\n",
              "                       Epitope - Source Molecule IRI  Epitope ID  \n",
              "0  PPPFGQAAAGDKPSPFGQAAAGDKPPPFGQAAAGDKPSPFGQAAAG...  AAP88022.1  \n",
              "1                          TYATGGAAARTTSGFASLFSPGSAQ  ABP93610.1  \n",
              "2  MTEQQWNFAGIEAAASAIQGNVTSIHSLLDEGKQSLTKLAAAWGGS...    P0A564.2  \n",
              "3  MSTLQELQENITAHEQQLVTARQKLKDAEKAVEVDPDDVNKSTLQN...  AAO86636.1  \n",
              "4  MRPRPILLLLLMFLPMLPAPPPGQPSGRRRGRRSGGSGGGFWGDRA...  AAA03191.1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"processed1.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "a4c97179",
      "metadata": {
        "id": "a4c97179"
      },
      "outputs": [],
      "source": [
        "def build_labels(sequence, start, end):\n",
        "    # Start with all 0s\n",
        "    labels = np.zeros(len(sequence), dtype=np.int64)\n",
        "    # assert epitope_end <= len(sequence) and epitope_start >= 0\n",
        "    labels[start: end] = 1  # 1 indicates epitope\n",
        "   \n",
        "    return labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ad7e7fd",
      "metadata": {
        "id": "5ad7e7fd"
      },
      "source": [
        "Now we've defined a helper function, let's build our lists of sequences and labels:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "313811fe",
      "metadata": {
        "id": "313811fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(533, 8)\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n",
            "249\n",
            "250\n",
            "251\n",
            "252\n",
            "253\n",
            "254\n",
            "255\n",
            "256\n",
            "257\n",
            "258\n",
            "259\n",
            "260\n",
            "261\n",
            "262\n",
            "263\n",
            "264\n",
            "265\n",
            "266\n",
            "267\n",
            "268\n",
            "269\n",
            "270\n",
            "271\n",
            "272\n",
            "273\n",
            "274\n",
            "275\n",
            "276\n",
            "277\n",
            "278\n",
            "279\n",
            "280\n",
            "281\n",
            "282\n",
            "283\n",
            "284\n",
            "285\n",
            "286\n",
            "287\n",
            "288\n",
            "289\n",
            "290\n",
            "291\n",
            "292\n",
            "293\n",
            "294\n",
            "295\n",
            "296\n",
            "297\n",
            "298\n",
            "299\n",
            "300\n",
            "301\n",
            "302\n",
            "303\n",
            "304\n",
            "305\n",
            "306\n",
            "307\n",
            "308\n",
            "309\n",
            "310\n",
            "311\n",
            "312\n",
            "313\n",
            "314\n",
            "315\n",
            "316\n",
            "317\n",
            "318\n",
            "319\n",
            "320\n",
            "321\n",
            "322\n",
            "323\n",
            "324\n",
            "325\n",
            "326\n",
            "327\n",
            "328\n",
            "329\n",
            "330\n",
            "331\n",
            "332\n",
            "333\n",
            "334\n",
            "335\n",
            "336\n",
            "337\n",
            "338\n",
            "339\n",
            "340\n",
            "341\n",
            "342\n",
            "343\n",
            "344\n",
            "345\n",
            "346\n",
            "347\n",
            "348\n",
            "349\n",
            "350\n",
            "351\n",
            "352\n",
            "353\n",
            "354\n",
            "355\n",
            "356\n",
            "357\n",
            "358\n",
            "359\n",
            "360\n",
            "361\n",
            "362\n",
            "363\n",
            "364\n",
            "365\n",
            "366\n",
            "367\n",
            "368\n",
            "369\n",
            "370\n",
            "371\n",
            "372\n",
            "373\n",
            "374\n",
            "375\n",
            "376\n",
            "377\n",
            "378\n",
            "379\n",
            "380\n",
            "381\n",
            "382\n",
            "383\n",
            "384\n",
            "385\n",
            "386\n",
            "387\n",
            "388\n",
            "389\n",
            "390\n",
            "391\n",
            "392\n",
            "393\n",
            "394\n",
            "395\n",
            "396\n",
            "397\n",
            "398\n",
            "399\n",
            "400\n",
            "401\n",
            "402\n",
            "403\n",
            "404\n",
            "405\n",
            "406\n",
            "407\n",
            "408\n",
            "409\n",
            "410\n",
            "411\n",
            "412\n",
            "413\n",
            "414\n",
            "415\n",
            "416\n",
            "417\n",
            "418\n",
            "419\n",
            "420\n",
            "421\n",
            "422\n",
            "423\n",
            "424\n",
            "425\n",
            "426\n",
            "427\n",
            "428\n",
            "429\n",
            "430\n",
            "431\n",
            "432\n",
            "433\n",
            "434\n",
            "435\n",
            "436\n",
            "437\n",
            "438\n",
            "439\n",
            "440\n",
            "441\n",
            "442\n",
            "443\n",
            "444\n",
            "445\n",
            "446\n",
            "447\n",
            "448\n",
            "449\n",
            "450\n",
            "451\n",
            "452\n",
            "453\n",
            "454\n",
            "455\n",
            "456\n",
            "457\n",
            "458\n",
            "459\n",
            "460\n",
            "461\n",
            "462\n",
            "463\n",
            "464\n",
            "465\n",
            "466\n",
            "467\n",
            "468\n",
            "469\n",
            "470\n",
            "471\n",
            "472\n",
            "473\n",
            "474\n",
            "475\n",
            "476\n",
            "477\n",
            "478\n",
            "479\n",
            "480\n",
            "481\n",
            "482\n",
            "483\n",
            "484\n",
            "485\n",
            "486\n",
            "487\n",
            "488\n",
            "489\n",
            "490\n",
            "491\n",
            "492\n",
            "493\n",
            "494\n",
            "495\n",
            "496\n",
            "497\n",
            "498\n",
            "499\n",
            "500\n",
            "501\n",
            "502\n",
            "503\n",
            "504\n",
            "505\n",
            "506\n",
            "507\n",
            "508\n",
            "509\n",
            "510\n",
            "511\n",
            "512\n",
            "513\n",
            "514\n",
            "515\n",
            "516\n",
            "517\n",
            "518\n",
            "519\n",
            "520\n",
            "521\n",
            "522\n",
            "523\n",
            "524\n",
            "525\n",
            "526\n",
            "527\n",
            "528\n",
            "529\n",
            "530\n",
            "531\n",
            "532\n"
          ]
        }
      ],
      "source": [
        "sequences = []\n",
        "labels = []\n",
        "print(df.shape)\n",
        "for index, row in df.iterrows():\n",
        "    sequence = row[\"Epitope - Source Molecule IRI\"]\n",
        "    epitope_start = int(row[\"Epitope - Starting Position\"]) - 1\n",
        "    epitope_end = int(row[\"Epitope - Ending Position\"])\n",
        "    print(index)\n",
        "    if epitope_end >= len(sequence) or epitope_start <= 0:\n",
        "        df.drop(index, inplace=True)\n",
        "    row_labels = build_labels(sequence, epitope_start, epitope_end)\n",
        "    sequences.append(row[\"Epitope - Source Molecule IRI\"])\n",
        "    labels.append(row_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e8b3ba8",
      "metadata": {
        "id": "8e8b3ba8"
      },
      "source": [
        "## Creating our dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e619d9ae",
      "metadata": {
        "id": "e619d9ae"
      },
      "source": [
        "Nice! Now we'll split and tokenize the data, and then create datasets - I'll go through this quite quickly here, since it's identical to how we did it in the sequence classification example above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "3c208c30",
      "metadata": {
        "id": "3c208c30"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_sequences, test_sequences, train_labels, test_labels = train_test_split(sequences, labels, test_size=0.15, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "2182fae2",
      "metadata": {
        "id": "2182fae2",
        "outputId": "277ddb32-6030-45e5-cd47-3a8a5e05e6a6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "train_tokenized = tokenizer(train_sequences)\n",
        "test_tokenized = tokenizer(test_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "3939f13a",
      "metadata": {
        "id": "3939f13a"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_dict(train_tokenized)\n",
        "test_dataset = Dataset.from_dict(test_tokenized)\n",
        "\n",
        "train_dataset = train_dataset.add_column(\"labels\", train_labels)\n",
        "test_dataset = test_dataset.add_column(\"labels\", test_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4766fe4b",
      "metadata": {
        "id": "4766fe4b"
      },
      "source": [
        "## Model loading"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8419b5",
      "metadata": {
        "id": "de8419b5"
      },
      "source": [
        "The key difference here with the above example is that we use `AutoModelForTokenClassification` instead of `AutoModelForSequenceClassification`. We will also need a `data_collator` this time, as we're in the slightly more complex case where both inputs and labels must be padded in each batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "4b26b828",
      "metadata": {
        "id": "4b26b828",
        "outputId": "cca58257-84a4-4dd4-c669-b0d963ee2cbd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of EsmForTokenClassification were not initialized from the model checkpoint at facebook/esm2_t12_35M_UR50D and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "num_labels = 2\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "eec0005a",
      "metadata": {
        "id": "eec0005a"
      },
      "outputs": [],
      "source": [
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd3c7305",
      "metadata": {
        "id": "bd3c7305"
      },
      "source": [
        "Now we set up our `TrainingArguments` as before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "e7724323",
      "metadata": {
        "id": "e7724323",
        "outputId": "38145342-79b7-412b-d88e-0e0b6c3ae8a0"
      },
      "outputs": [],
      "source": [
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "batch_size = 8\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-epitope\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=1e-4,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.001,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: evaluate in ./venv/lib/python3.11/site-packages (0.4.1)\n",
            "Requirement already satisfied: datasets>=2.0.0 in ./venv/lib/python3.11/site-packages (from evaluate) (2.15.0)\n",
            "Requirement already satisfied: numpy>=1.17 in ./venv/lib/python3.11/site-packages (from evaluate) (1.23.5)\n",
            "Requirement already satisfied: dill in ./venv/lib/python3.11/site-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: pandas in ./venv/lib/python3.11/site-packages (from evaluate) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in ./venv/lib/python3.11/site-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in ./venv/lib/python3.11/site-packages (from evaluate) (4.65.0)\n",
            "Requirement already satisfied: xxhash in ./venv/lib/python3.11/site-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in ./venv/lib/python3.11/site-packages (from evaluate) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in ./venv/lib/python3.11/site-packages (from evaluate) (2023.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in ./venv/lib/python3.11/site-packages (from evaluate) (0.19.4)\n",
            "Requirement already satisfied: packaging in ./venv/lib/python3.11/site-packages (from evaluate) (23.0)\n",
            "Requirement already satisfied: responses<0.19 in ./venv/lib/python3.11/site-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (14.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (0.5)\n",
            "Requirement already satisfied: aiohttp in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./venv/lib/python3.11/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: filelock in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./venv/lib/python3.11/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./venv/lib/python3.11/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in ./venv/lib/python3.11/site-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./venv/lib/python3.11/site-packages (from pandas->evaluate) (2023.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./venv/lib/python3.11/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.11/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5fba9a",
      "metadata": {
        "id": "fb5fba9a"
      },
      "source": [
        "Our `compute_metrics` function is a bit more complex than in the sequence classification task, as we need to ignore padding tokens (those where the label is `-100`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "736886a0",
      "metadata": {
        "id": "736886a0"
      },
      "outputs": [],
      "source": [
        "\n",
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "metric = load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    labels = labels.reshape((-1,))\n",
        "    predictions = np.argmax(predictions, axis=2)\n",
        "    predictions = predictions.reshape((-1,))\n",
        "    predictions = predictions[labels!=-100]\n",
        "    labels = labels[labels!=-100]\n",
        "    return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37491af5",
      "metadata": {
        "id": "37491af5"
      },
      "source": [
        "And now we're ready to train our model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "4c97836c",
      "metadata": {
        "id": "4c97836c",
        "outputId": "ac7d2c37-82b0-4dc0-9ec2-92a106ecff82"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9eac1e589e894c07af0ff24da5930f93",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/171 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/amalicema24/Desktop/epitope/venv/lib/python3.11/site-packages/transformers/models/esm/modeling_esm.py:219: UserWarning: MPS: no support for int64 for sum_out_mps, downcasting to a smaller data type (int32/float32). Native support for int64 has been added in macOS 13.3. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/mps/operations/ReduceOps.mm:155.)\n",
            "  src_lengths = attention_mask.sum(-1)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/Users/amalicema24/Desktop/epitope/attempt1.ipynb Cell 39\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amalicema24/Desktop/epitope/attempt1.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amalicema24/Desktop/epitope/attempt1.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     model,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amalicema24/Desktop/epitope/attempt1.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amalicema24/Desktop/epitope/attempt1.ipynb#X53sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     data_collator\u001b[39m=\u001b[39mdata_collator,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/amalicema24/Desktop/epitope/attempt1.ipynb#X53sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/amalicema24/Desktop/epitope/attempt1.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[0;32m~/Desktop/epitope/venv/lib/python3.11/site-packages/transformers/trainer.py:1546\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1543\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     \u001b[39m# Disable progress bars when uploading models during checkpoints to avoid polluting stdout\u001b[39;00m\n\u001b[1;32m   1545\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39mdisable_progress_bars()\n\u001b[0;32m-> 1546\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1547\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1548\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1549\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1550\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1551\u001b[0m     )\n\u001b[1;32m   1552\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1553\u001b[0m     hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n",
            "File \u001b[0;32m~/Desktop/epitope/venv/lib/python3.11/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[39mif\u001b[39;00m step \u001b[39m%\u001b[39m args\u001b[39m.\u001b[39mgradient_accumulation_steps \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1857\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 1859\u001b[0m \u001b[39mwith\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49maccumulate(model):\n\u001b[1;32m   1860\u001b[0m     tr_loss_step \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[1;32m   1862\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   1863\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1864\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1865\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1866\u001b[0m ):\n\u001b[1;32m   1867\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n",
            "File \u001b[0;32m~/.pyenv/versions/3.11.3/lib/python3.11/contextlib.py:141\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__exit__\u001b[0;34m(self, typ, value, traceback)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    139\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, typ, value, traceback):\n\u001b[1;32m    142\u001b[0m     \u001b[39mif\u001b[39;00m typ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "results = trainer.evaluate()\n",
        "print(results)\n",
        "\n",
        "# Save the model weights\n",
        "model_output_dir = \"model\"\n",
        "trainer.save_model(model_output_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f503fc00",
      "metadata": {
        "id": "f503fc00"
      },
      "source": [
        "This definitely seems harder than the first task, but we still attain a very respectable accuracy. Remember that to keep this demo lightweight, we used one of the smallest ESM models, focused on human proteins only and didn't put a lot of work into making sure we only included completely-annotated proteins in our training set. With a bigger model and a cleaner, broader training set, accuracy on this task could definitely go a lot higher!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
